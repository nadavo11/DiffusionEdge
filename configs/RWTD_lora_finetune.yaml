# RWTD LoRA Fine-Tuning Config (Fast Smoke Experiment)
# Based on RWTD_finetune_from_BSDS_train.yaml with LoRA enabled and short training.
#
# Fast run:    accelerate launch train_cond_ldm.py --cfg configs/RWTD_lora_finetune.yaml
# Dry run:     python train_cond_ldm.py --cfg configs/RWTD_lora_finetune.yaml --dry_run_lora
# Longer run:  edit trainer.train_num_steps below (e.g. 10000)

model:
  model_type: const_sde
  model_name: cond_unet
  image_size: [320, 320]
  input_keys: ['image', 'cond']
  ckpt_path: './training/synthetic_food_stitched/model-last.pt'
  ignore_keys: [ ]
  only_model: False
  timesteps: 1000
  train_sample: -1
  sampling_timesteps: 1
  loss_type: l2
  objective: pred_KC
  start_dist: normal
  perceptual_weight: 0
  scale_factor: 0.3
  scale_by_std: True
  default_scale: True
  scale_by_softsign: False
  eps: !!float 1e-4
  weighting_loss: True
  use_disloss: True

  first_stage:
    embed_dim: 3
    lossconfig:
      disc_start: 50001
      kl_weight: 0.000001
      disc_weight: 0.5
      disc_in_channels: 1
    ddconfig:
      double_z: True
      z_channels: 3
      resolution: [320, 320]
      in_channels: 1
      out_ch: 1
      ch: 128
      ch_mult: [1, 2, 4]
      num_res_blocks: 2
      attn_resolutions: [ ]
      dropout: 0.0
    ckpt_path: './checkpoints/first_stage_total_320.pt'

  unet:
    dim: 128
    cond_net: swin
    fix_bb: False
    channels: 3
    out_mul: 1
    dim_mults: [1, 2, 4, 4]
    cond_in_dim: 3
    cond_dim: 128
    cond_dim_mults: [2, 4]
    window_sizes1: [[8, 8], [4, 4], [2, 2], [1, 1]]
    window_sizes2: [[8, 8], [4, 4], [2, 2], [1, 1]]
    fourier_scale: 16
    cond_pe: False
    num_pos_feats: 128
    cond_feature_size: [80, 80]
    input_size: [80, 80]

# --- LoRA config ---
lora:
  enabled: true
  r: 2048
  alpha: 2048
  dropout: 0.05
  targets: "q_lin,k_lin,v_lin"
  save_lora_only: true
  ckpt_path: null   # set to resume from a lora checkpoint

data:
  name: edge
  crop_type: rand_resize_crop
  img_folder: 'RWTD_5050'
  train_split: train
  eval_split: test
  strict_split: True
  augment_horizontal_flip: True
  batch_size: 2
  num_workers: 4

trainer:
  gradient_accumulate_every: 4
  lr: !!float 5e-5
  min_lr: !!float 5e-6
  train_num_steps: 5000          # <-- fast smoke: 200 steps, increase for real runs
  save_and_sample_every: 100
  enable_resume: False
  log_freq: 10
  results_folder: "./training/rwtd_lora_finetune"
  checkpoint_folder: "./training/rwtd_lora_finetune/checkpoints"
  amp: False
  fp16: False
  resume_milestone: 0
  test_before: True
  ema_update_after_step: 5000
  ema_update_every: 10
  eval:
    enabled: True
    run_at_step0: True
    every_steps: 100
    split: test
    strict_split: True
    batch_size: 1
    num_workers: 2
    num_batches: 32
    mode: edge
    thresholds: 99
    max_dist: 0.0075
    apply_nms: True
    apply_thinning: True
    nproc: 4
    preview_limit: 8
    select_best_metric: edge/AP
    rwtd:
      enabled: False
